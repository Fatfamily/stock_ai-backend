from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
import httpx
import os
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# -------------------------------------
# ì´ˆê¸° ì„¤ì •
# -------------------------------------

load_dotenv()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

app = FastAPI()

# CORS (Flutter ì•±ì—ì„œ í˜¸ì¶œ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],      # í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

NAVER_NEWS_URL = "https://openapi.naver.com/v1/search/news.json"


# -------------------------------------
# ê³µí†µ: ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜
# -------------------------------------

async def fetch_news(keyword: str, limit: int = 30, sort: str = "date"):
    """
    keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¢…ëª©ëª…, í‚¤ì›Œë“œ, ìì—°ì–´ ë“±)
    limit: ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
    sort: 'date' or 'sim'
    """

    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        raise HTTPException(500, "NAVER API key missing (.env í™•ì¸ í•„ìš”)")

    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }

    params = {
        "query": keyword,
        "display": min(limit, 100),
        "sort": sort  # 'date' = ìµœì‹ ìˆœ, 'sim' = ê´€ë ¨ë„/ì¸ê¸°ìˆœ ëŠë‚Œ
    }

    async with httpx.AsyncClient() as client:
        res = await client.get(NAVER_NEWS_URL, headers=headers, params=params)

    if res.status_code != 200:
        raise HTTPException(res.status_code, res.text)

    data = res.json()

    articles = [
        {
            "title": item.get("title"),
            "desc": item.get("description"),
            "link": item.get("link"),
            "pubDate": item.get("pubDate"),
        }
        for item in data.get("items", [])
    ]

    return {
        "keyword": keyword,
        "count": len(articles),
        "articles": articles
    }


# -------------------------------------
# ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# -------------------------------------

async def fetch_article_content(url: str):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ ì œëª© + ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    (ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì•„ë‹ ê²½ìš° ìµœëŒ€í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ)
    """
    async with httpx.AsyncClient() as client:
        res = await client.get(url)

    if res.status_code != 200:
        raise HTTPException(res.status_code, f"ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨: {res.text}")

    html = res.text
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª©
    title = soup.title.string.strip() if soup.title and soup.title.string else ""

    content_text = ""

    # ë„¤ì´ë²„ ë‰´ìŠ¤(ìƒˆ UI)ì˜ ê²½ìš° ë³´í†µ #dic_area ì•ˆì— ë³¸ë¬¸ì´ ìˆìŒ
    main_area = soup.select_one("#dic_area")
    if main_area:
        paragraphs = [
            p.get_text(strip=True)
            for p in main_area.find_all(["p", "span"])
            if p.get_text(strip=True)
        ]
        content_text = "\n".join(paragraphs)

    # fallback: ê·¸ë˜ë„ ë¹„ì–´ ìˆìœ¼ë©´ í˜ì´ì§€ ì „ì²´ì—ì„œ p íƒœê·¸ ê¸ê¸°
    if not content_text:
        ps = soup.find_all("p")
        paragraphs = [
            p.get_text(strip=True)
            for p in ps
            if p.get_text(strip=True)
        ]
        content_text = "\n".join(paragraphs[:30])  # ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ 30ê°œê¹Œì§€

    if not title and not content_text:
        raise HTTPException(500, "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    return {
        "url": url,
        "title": title,
        "content": content_text,
        "origin_link": url,
    }


# -------------------------------------
# API ì—”ë“œí¬ì¸íŠ¸
# -------------------------------------

@app.get("/")
def home():
    return {
        "status": "OK",
        "message": "Stock AI backend running",
        "endpoints": ["/hot-news", "/news", "/article"]
    }


# ğŸ”¹ 1) í•« ë‰´ìŠ¤ (ì•± ì²« í™”ë©´ìš©)
@app.get("/hot-news")
async def get_hot_news(
    limit: int = Query(5, ge=1, le=50, description="ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜"),
    sort: str = Query("popular", description="latest ë˜ëŠ” popular")
):
    """
    ì•± ì²« ì ‘ì† ì‹œ ì‚¬ìš©:
    - ê¸°ë³¸ 5ê°œ
    - ë”ë³´ê¸° ëˆŒë €ì„ ë•Œ 10, 20 ë“±ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥
    sort:
      - latest -> ìµœì‹ ìˆœ (date)
      - popular -> ì¸ê¸°/ê´€ë ¨ë„ìˆœ ëŠë‚Œ (sim)
    """
    # ì •ë ¬ ì˜µì…˜ ë§¤í•‘
    if sort in ["latest", "date", "time"]:
        naver_sort = "date"
    else:  # 'popular' ë˜ëŠ” ê¸°íƒ€
        naver_sort = "sim"

    # ì—¬ê¸°ì„  ì „ì²´ "ì£¼ì‹" ê´€ë ¨ í•« ë‰´ìŠ¤ë¼ê³  ê°€ì •
    # í•„ìš”í•˜ë©´ "ì¦ê¶Œ", "ì½”ìŠ¤í”¼" ë“± í‚¤ì›Œë“œ ì¡°í•©í•´ì„œ í™•ì¥ ê°€ëŠ¥
    return await fetch_news("ì£¼ì‹", limit=limit, sort=naver_sort)


# ğŸ”¹ 2) ê²€ìƒ‰ìš© ë‰´ìŠ¤ ì—”ë“œí¬ì¸íŠ¸
@app.get("/news")
async def search_news(
    keyword: str = Query(..., description="ì¢…ëª©ëª… ë˜ëŠ” í‚¤ì›Œë“œ (ì˜ˆ: ì‚¼ì„±ì „ì, AI, ë°˜ë„ì²´, ì‚¼ì„± AI ë“±)"),
    limit: int = Query(30, ge=1, le=100),
    sort: str = Query("latest", description="latest | popular")
):
    """
    ê²€ìƒ‰ì°½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
    - keyword: ììœ  ê²€ìƒ‰ (ì¢…ëª©ëª…, í‚¤ì›Œë“œ, ìì—°ì–´ ë‹¤ ê°€ëŠ¥)
    - sort:
        latest  -> ìµœì‹ ìˆœ (date)
        popular -> ê´€ë ¨ë„/ì¸ê¸°ìˆœ ëŠë‚Œ (sim)
    """
    if sort in ["latest", "date", "time"]:
        naver_sort = "date"
    else:  # popular
        naver_sort = "sim"

    return await fetch_news(keyword, limit=limit, sort=naver_sort)


# ğŸ”¹ 3) ê¸°ì‚¬ ìƒì„¸ ë³´ê¸° (ë³¸ë¬¸ + ë§í¬)
@app.get("/article")
async def get_article(url: str = Query(..., description="ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URL")):
    """
    ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œëª© í´ë¦­í–ˆì„ ë•Œ
    - ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸
    - ì œëª©
    - ì›ë¬¸ ë§í¬
    ë¥¼ ë°˜í™˜í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
    """
    return await fetch_article_content(url)
